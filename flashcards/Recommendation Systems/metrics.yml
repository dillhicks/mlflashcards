subject: Metrics
description: Key metrics for evaluating the performance and quality of recommender systems.
cards:
  - front: |
      # How do I choose the **right metric** for my recommender system?
    back: |
      You should choose your metric based on the primary goal of the recommendation task.

      | Use Case / Goal | Key Question | Common Metrics |
      |---|---|---|
      | **Checking Basic Relevancy** | Are the recommended items good, regardless of order? | **Precision@K, Recall@K** |
      | **Evaluating Ranking Quality** | Are the *best* items ranked at the top of the list? | **MAP, NDCG, MRR** |
      | **Measuring User Experience** | Is the system interesting and not just showing popular items? | **Coverage, Novelty, Serendipity** |

  - front: |
      # **Precision@K**
    back: |
      - **What it is**: A relevancy metric that measures the proportion of recommended items in the top-K set that are actually relevant.
      
      - **What it's used for**: To evaluate the **immediate accuracy** of the recommendations shown to the user, especially when the order of those items doesn't matter much. It answers the question, "Out of the K items I showed, how many were hits?"

      - **Example Use Cases**:
        - **E-commerce Homepage**: "Of the 10 products shown in the 'For You' widget, how many were actually good suggestions?"
        - **YouTube Sidebar**: "Are the 5 videos recommended next to the current one relevant?"

  - front: |
      # **Formula**: Precision@K
    back: |
      Precision at K is the number of relevant recommendations in the top K results, divided by K.

      ## Formula:
      `Precision@K = (Number of relevant items in the top K) / K`

      ---
      ### Example:
      If a user is recommended 10 items (`K=10`) and 4 of them are relevant:
      
      `Precision@10 = 4 / 10 = 0.4`

  - front: |
      # **Recall@K**
    back: |
      - **What it is**: A relevancy metric that measures the proportion of *all possible relevant items* that are successfully recommended in the top-K set.

      - **What it's used for**: To evaluate how well the system can **find and present the full range** of items a user might be interested in. It's important when missing a relevant item is costly.

      - **Example Use Cases**:
        - **Movie Recommendations**: "For a user who loves sci-fi, did the system manage to recommend most of the classic sci-fi movies in its top 50 suggestions?"
        - **Job Search**: "Out of all the suitable jobs in the database, what percentage did the recommender show me on the first page?"

  - front: |
      # **Formula**: Recall@K
    back: |
      Recall at K is the number of relevant recommendations in the top K results, divided by the total number of relevant items that exist for that user.

      ## Formula:
      `Recall@K = (Number of relevant items in the top K) / (Total number of relevant items)`

      ---
      ### Example:
      A user has 5 total movies they would find relevant. The system recommends 10 movies, and 4 of them are from that relevant set.
      
      `Recall@10 = 4 / 5 = 0.8`

  - front: |
      # **Mean Average Precision (MAP)**
    back: |
      - **What it is**: A **ranking metric** that heavily rewards putting relevant items at the top of the list. It's an extension of Precision@K that cares deeply about the order.

      - **What it's used for**: To evaluate the quality of an **ordered list** of recommendations, where the position of each relevant item significantly impacts the user experience.

      - **Example Use Cases**:
        - **Internal Search Engine**: "When an employee searches for 'PTO policy', the official document should be the #1 result, not #8. MAP would heavily penalize the model for putting it lower."
        - **Spotify Playlist Generator**: "When creating a 'Focus Mix', the best, most calming songs should appear early in the playlist. MAP would reward this."

  - front: |
      # **Formula**: Mean Average Precision (MAP)
    back: |
      MAP is the **mean** of the **Average Precision (AP)** scores calculated for each user.

      ### Average Precision (AP) for a single user:
      AP is the average of the Precision@k scores calculated at each position `k` where a relevant item was found.

      `AP = (1/R) * Σ (P(k) * rel(k))`
      - `R`: Total number of relevant items.
      - `P(k)`: Precision at position `k`.
      - `rel(k)`: An indicator function that is 1 if the item at `k` is relevant, and 0 otherwise.

      ### MAP Formula:
      `MAP = (1/N) * Σ (APₙ)`
      - `N`: Total number of users.
      - `APₙ`: The Average Precision for user `n`.

  - front: |
      # **Normalized Discounted Cumulative Gain (NDCG)**
    back: |
      - **What it is**: A sophisticated **ranking metric** that not only rewards putting relevant items high up but can also handle different **levels of relevance**.

      - **What it's used for**: When recommendations aren't just 'relevant' or 'not relevant', but can be rated on a scale (e.g., 'perfect', 'good', 'okay', 'bad'). It's the gold standard for evaluating ranked lists with graded relevance.

      - **Example Use Cases**:
        - **Hotel Search**: "A hotel that perfectly matches all criteria (price, location, amenities) is more valuable than one that only matches some. NDCG can assign a higher score to the 'perfect' match and reward the system more for ranking it first."
        - **E-commerce Search**: For "blue running shoes," a pair of blue Nikes is more relevant than blue sneakers. NDCG understands these grades of relevance.

  - front: |
      # **Formula**: Normalized Discounted Cumulative Gain (NDCG)
    back: |
      NDCG is the **Discounted Cumulative Gain (DCG)** of a recommended list, divided by the **Ideal Discounted Cumulative Gain (IDCG)**. This normalizes the score to be between 0 and 1.

      ### Discounted Cumulative Gain (DCG):
      DCG sums up the relevance scores of the items in the list, but penalizes (discounts) the scores of items that appear lower down.

      `DCG@k = Σ (relᵢ / log₂(i+1))`
      - `relᵢ`: The graded relevance score of the item at position `i`.
      - `log₂(i+1)`: The discount factor.

      ### NDCG Formula:
      `NDCG@k = DCG@k / IDCG@k`
      - `IDCG@k`: The DCG score of the *perfectly* ranked list (all relevant items sorted by relevance score at the top).

  - front: |
      # **Mean Reciprocal Rank (MRR)**
    back: |
      - **What it is**: A simple **ranking metric** that only cares about the rank of the **first** relevant item in the list of recommendations.

      - **What it's used for**: When the user's goal is to **find one single correct answer or item**, and they are likely to stop searching once they find it.

      - **Example Use Cases**:
        - **Question Answering / "People also ask"**: The user just wants the correct answer. MRR measures how quickly the system provides it.
        - **Navigational Search**: "When a user types 'Netflix login' into a search bar, they only care about finding the correct login page. The rank of the first correct link is all that matters."

  - front: |
      # **Formula**: Mean Reciprocal Rank (MRR)
    back: |
      MRR is the average of the reciprocal ranks for a set of queries. The **reciprocal rank** for a single query is the multiplicative inverse of the rank of the first correct answer.

      ## Reciprocal Rank for one user:
      `1 / rank₁`
      - `rank₁`: The position of the first relevant item in the recommended list.

      ## MRR Formula:
      `MRR = (1/N) * Σ (1 / rankᵢ)`
      - `N`: The total number of users or queries.

      If the first relevant item for a user is at position 3, their reciprocal rank is `1/3`. MRR is the average of these scores across all users.

  - front: |
      # **Coverage**
    back: |
      - **What it is**: A metric that measures the percentage of all items in the catalog that the recommender system is capable of recommending.

      - **What it's used for**: To diagnose if the system is suffering from a "popularity bias" where it only recommends popular 'head' items and ignores the 'long tail' of the catalog.

      - **Example Use Cases**:
        - **Bookstore Website**: "Is the system only ever recommending bestsellers, or does it also recommend niche, indie author books? High coverage means more of the catalog is discoverable."

  - front: |
      # **Novelty** & **Serendipity**
    back: |
      ## Novelty
      - **What it is**: Measures how **new or unfamiliar** the recommended items are to a user.
      - **Use Case**: To ensure the system is introducing users to new content, not just recommending things they already know and like.

      ## Serendipity
      - **What it is**: Measures how **unexpected and pleasantly surprising** the recommendations are. A serendipitous item is both novel and relevant in an unobvious way.
      - **Use Case**: To evaluate the system's ability to "delight" the user by helping them discover new interests, which is key for long-term engagement.